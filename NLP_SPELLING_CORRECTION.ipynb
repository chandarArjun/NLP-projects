{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP SPELLING CORRECTION.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Python Spelling Corrector\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "\n",
        "\n",
        "def make_dictionary(dictionary_file):\n",
        "    with open(dictionary_file) as f:\n",
        "        english = []\n",
        "        for line in f:\n",
        "            line = line[:-1]\n",
        "            english.append(line)\n",
        "            pass\n",
        "        pass\n",
        "    return english\n",
        "\n",
        "\n",
        "def tokenization(text_string):\n",
        "    word_list = re.split(\"(\\W+)\", text_string)\n",
        "    return word_list\n",
        "\n",
        "\n",
        "def detokenization(word_list):\n",
        "    doc = \"\"\n",
        "    doc = doc.join(word_list)\n",
        "    return doc\n",
        "\n",
        "\n",
        "def calc_distance(word, word2):\n",
        "    rows = len(word) + 1\n",
        "    columns = len(word2) + 1\n",
        "    lev = np.zeros((rows, columns)).astype(int)\n",
        "    for i in range(rows):\n",
        "        lev[i, 0] = i\n",
        "        pass\n",
        "    for i in range(columns):\n",
        "        lev[0, i] = i\n",
        "        pass\n",
        "    for i in range(1, rows):\n",
        "        for j in range(1, columns):\n",
        "            if word[i - 1] == word2[j - 1]:\n",
        "                lev[i, j] = min(lev[i - 1, j] + 1, lev[i, j - 1] + 1, lev[i - 1, j - 1])\n",
        "                pass\n",
        "            else:\n",
        "                lev[i, j] = min(\n",
        "                    lev[i - 1, j] + 1, lev[i, j - 1] + 1, lev[i - 1, j - 1] + 1\n",
        "                )\n",
        "                pass\n",
        "            pass\n",
        "        pass\n",
        "    return lev[rows - 1, columns - 1]\n",
        "\n",
        "\n",
        "def check_word(index, list, english):\n",
        "    if len(list[index]) == 0:\n",
        "        return False\n",
        "    elif (\n",
        "        not re.fullmatch(\"(\\W+)\", list[index])\n",
        "        and list[index] not in english\n",
        "        and not list[index][0].isupper()\n",
        "        and not list[index][0].isnumeric()\n",
        "    ):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def spelling_corrector(text_string, dictionary_file):\n",
        "    english = make_dictionary(dictionary_file)\n",
        "    word_list = tokenization(text_string)\n",
        "    for i in range(len(word_list)):\n",
        "        if check_word(i, word_list, english):\n",
        "            distlist = []\n",
        "            for j in english:\n",
        "                distlist.append(calc_distance(word_list[i], j))\n",
        "                pass\n",
        "            replace = english[distlist.index(min(distlist))]\n",
        "            word_list[i] = replace\n",
        "            pass\n",
        "        pass\n",
        "    doc = detokenization(word_list)\n",
        "    return doc\n",
        "    pass"
      ],
      "metadata": {
        "id": "UwAXWFwqpN4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyautocorrect\n",
        "\n",
        "text = \"/content/drive/MyDrive/Corrupted text.txt\"\n",
        "with open(text) as f:\n",
        "    text_string = f.read()\n",
        "output = pyautocorrect.correct(text_string, text)\n",
        "f = open(text, \"w\")\n",
        "f.write(output)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "ImlSYTnizfp1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}